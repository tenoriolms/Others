{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.python.org/pt-br/3/tutorial/modules.html"
      ],
      "metadata": {
        "id": "yhh50b4DO_yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import missingno as msno\n",
        "\n",
        "\n",
        "import sklearn.metrics"
      ],
      "metadata": {
        "id": "4OBUth-Cre0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import sklearn.datasets\n",
        "import sklearn.model_selection\n",
        "\n",
        "import sklearn.ensemble #bibliotecas de aprendizado de máquina\n",
        "'''"
      ],
      "metadata": {
        "id": "lNstVHDPrkZ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a4c51ff3-b6a2-488a-9e92-dccabc62f14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport sklearn.datasets\\nimport sklearn.model_selection\\n\\nimport sklearn.ensemble #bibliotecas de aprendizado de máquina\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Declaração de Variáveis"
      ],
      "metadata": {
        "id": "OJgsy03zYNiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = '962' #ID do databank\n",
        "git_import_file = '' #nome da variante do databank original\n",
        "data_url = ''\n",
        "\n",
        "\n",
        "#dict para armazenar dados da função ID\n",
        "ID_dict = {}\n",
        "\n",
        "#gases com permeabilidades presentes no banco de dados ordem crescente de kinetic diameter\n",
        "#EM ORDEM CRESCENTE AO D.C., COMO NO DATABANK:\n",
        "gases = ('He','H2','CO2','O2','H2S','N2','CO','CH4','C2H6','SF6') #referentes aos dados de permeabilidade\n",
        "gases_kinetic_diameter = {'He':260,\n",
        "                          'H2':289,\n",
        "                          'CO2':330,\n",
        "                          'Ar':340,\n",
        "                          'O2':346,\n",
        "                          'H2S':360,\n",
        "                          'N2':364,\n",
        "                          'CO':376,\n",
        "                          'CH4':380,\n",
        "                          'C2H6':444.3,\n",
        "                          'SF6':550,\n",
        "                          'none':0}\n",
        "gases_effec_diameter = {'He':178, #T>Tg\n",
        "                        'H2':214,\n",
        "                        'CO2':302,\n",
        "                        'O2':289,\n",
        "                        'Ar':297,\n",
        "                        'H2S':np.nan,\n",
        "                        'N2':304,\n",
        "                        'CO':304,\n",
        "                        'CH4':318,\n",
        "                        'C2H6':346,\n",
        "                        'SF6':np.nan,\n",
        "                        'none':0}\n",
        "gases_molar_mass = {'He':4.00, \n",
        "                    'H2':2.02,\n",
        "                    'CO2':44.01,\n",
        "                    'O2':31.98,\n",
        "                    'Ar':39.95,\n",
        "                    'H2S':34.10,\n",
        "                    'N2':28.00,\n",
        "                    'CO':28.01,\n",
        "                    'CH4':16.04,\n",
        "                    'C2H6':30.07,\n",
        "                    'SF6':146.06,\n",
        "                    'none':0.}\n",
        "gases_polarizability = {'He':0.208, #https://cccbdb.nist.gov/pollistx.asp\n",
        "                        'H2':0.787,\n",
        "                        'CO2':2.507,\n",
        "                        'O2':1.562,\n",
        "                        'Ar':1.664,\n",
        "                        'H2S':3.631,\n",
        "                        'N2':1.710,\n",
        "                        'CO':1.953,\n",
        "                        'CH4':2.448,\n",
        "                        'C2H6':4.226,\n",
        "                        'SF6':4.490,\n",
        "                        'none':0.}\n",
        "\n",
        "\n",
        "gases_kinetic_diameter_inverse = {}\n",
        "for i in gases_kinetic_diameter.keys():\n",
        "  gases_kinetic_diameter_inverse[ gases_kinetic_diameter[i] ] = i\n",
        "\n",
        "gases_effec_diameter_inverse = {}\n",
        "for i in gases_effec_diameter.keys():\n",
        "  gases_effec_diameter_inverse[ gases_effec_diameter[i] ] = i\n",
        "                        \n",
        "#frequencia de dados para cada gas em situação \"pura\" e \"mixtura\"\n",
        "count_pure = {}\n",
        "count_mixture = {}\n",
        "\n",
        "\n",
        "#nome das colunas correspondentes à características da membrana e do processo e \n",
        "#seus respectivos índices na coluna\n",
        "#EM ORDEM:\n",
        "columns_membrane = ['type', 'description', 'support_material', 'subtype', 'filler_loading',\n",
        "                    'previous_thickness', 'mean_thickness', 'mean_pore_size', 'pore_volume',\n",
        "                    'specific_surface_area',  'aging']\n",
        "columns_process = ['surface_area', 'temperature', 'feed_pressure', 'permeate_pressure',\n",
        "                   'delta_pressure', 'feed_flow_rate', 'sweep_gas', 'sweep_gas_flow',\n",
        "                   'stage_cut']\n",
        "columns_others = ['provided_data_type', 'in_reference_data_location', 'reference', 'url']\n",
        "\n",
        "columns_membrane_index = {}\n",
        "columns_process_index = {}\n",
        "columns_others_index = {}\n",
        "\n",
        "#A performance de cada gás foi representada pela seguintes variáveis:\n",
        "prefix1='x_' #Fração mássica/molar/volumétrica\n",
        "prefix2='Py_' #Permeabilidade\n",
        "prefix3='Pe_' #Permeância\n",
        "\n",
        "#VARIÁVEIS AUXILIARES:\n",
        "dados = pd.DataFrame() #Dataframe a ser utilizados para a previsão. Versão refinada."
      ],
      "metadata": {
        "id": "lF5GegZeiqgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GitHub info"
      ],
      "metadata": {
        "id": "7hw5ZX-UrwPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git_username = 'tenoriolms' #username no GitHub\n",
        "git_repository = 'databank_CH4' #Nome do repositório\n",
        "\n",
        "git_token = 'não pode' #token para acesso do repositório.\n",
        "#O github possui um algoritmo para verificar se dentro de cada arquivo importado/commitado\n",
        "#existe o token de acesso criado, que é secreto. Caso existir, esse token é revogado.\n",
        "#Como esse notebook irá ser exportado para o github. O token não pode ser escrito aqui.\n",
        "\n",
        "!git config --global user.email \"lhucas_tenorio@hotmail.com\"\n",
        "!git config --global user.name \"tenoriolms\""
      ],
      "metadata": {
        "id": "8b3c8hLXgxzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Funções"
      ],
      "metadata": {
        "id": "FCyT9E2Q5mgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ID(index)"
      ],
      "metadata": {
        "id": "vtgMNNwklJU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ID = string utilizada para identificar uma membrana e suas circunstâncias de utilização no banco de dados\n",
        "#A ID é a soma das variáveis (em forma de strings) que podem variar em uma dada referência\n",
        "def ID(i, df):\n",
        "  if i in ID_dict:\n",
        "    return ID_dict[i]\n",
        "  else:\n",
        "    \n",
        "    aux = str(df['description'][i]) + str(df['filler_loading'][i]) \\\n",
        "    + str(df['mean_thickness'][i]) + str(df['mean_pore_size'][i]) \\\n",
        "    + str(df['temperature'][i]) + str(df['feed_pressure'][i]) \\\n",
        "    + str(df['delta_pressure'][i]) + str(df['feed_flow_rate'][i]) \\\n",
        "    + str(df['stage_cut'][i]) + str(df['aging'][i]) \\\n",
        "    + str(df['reference'][i])\n",
        "    ID_dict[i] = aux\n",
        "    \n",
        "    return aux"
      ],
      "metadata": {
        "id": "Dl2BDR83sBUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##get_key(val,my_dict))"
      ],
      "metadata": {
        "id": "bAibWU-Ace4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_key(val,my_dict): #dict = key : value\n",
        "    for key, value in my_dict.items():\n",
        "         if val == value:\n",
        "             return key\n",
        " \n",
        "    return \"get_key function: There is no such Key\""
      ],
      "metadata": {
        "id": "CeZ4uSBD0e8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##submit_file(git_export_file)"
      ],
      "metadata": {
        "id": "g4VQjLibWZFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exportar para o GitHub\n",
        "def submit_file(git_export_file):\n",
        "  !git clone https://{git_token}@github.com/{git_username}/{git_repository}\n",
        "  !cp {git_export_file} {git_repository}\n",
        "  %cd {git_repository}\n",
        "  !git add {git_export_file}\n",
        "  !git commit -m 'Add/Atualizar arquivo {input_file}'\n",
        "  !git push -u origin\n",
        "  %cd ..\n",
        "  !rm -rf {git_repository}"
      ],
      "metadata": {
        "id": "xDNh_KvisG17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##import_file(git_import_file)"
      ],
      "metadata": {
        "id": "Uy4iamAaehgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importar um arquivo do GitHub\n",
        "def import_file(git_import_file):\n",
        "  !git clone https://{git_token}@github.com/{git_username}/{git_repository}\n",
        "  !cp {git_repository}/{git_import_file} .\n",
        "  !rm -rf {git_repository}"
      ],
      "metadata": {
        "id": "xJnMRW1vsXxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zscores(df_for_scaled, df_reference)"
      ],
      "metadata": {
        "id": "YTXQNw2ofLyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Escalonar cada coluna utilizando o \"z score\"\n",
        "def Zscores(df_for_scaled, df_reference):\n",
        "  if (any(df_for_scaled.columns != df_reference.columns)):\n",
        "    print('Zscores function: Dataframes com colunas diferentes')\n",
        "    return\n",
        "  \n",
        "  print(f'Zscores function: columns_reference: {df_reference.columns}')\n",
        "  for i in df_for_scaled.columns: \n",
        "    df_for_scaled[i] = (df_for_scaled[i] - df_reference[i].mean()) / df_reference[i].std()\n",
        "  #return df_for_scaled"
      ],
      "metadata": {
        "id": "eJSvt3-Nn1ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##undo_Zscores(df_scaled, df_reference)"
      ],
      "metadata": {
        "id": "3CZULGDEKYG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#desfazer o escalonamento realizado para cada coluna utilizando o \"z score\"\n",
        "def undo_Zscores(df_scaled, df_reference):\n",
        "  if (any(df_scaled.columns != df_reference.columns)):\n",
        "    print('undo_Zscores function: Dataframes com colunas diferentes')\n",
        "    return\n",
        "  \n",
        "  print(f'undo_Zscores function: columns_reference: {df_reference.columns}')\n",
        "  for i in df_scaled.columns:\n",
        "    df_scaled[i] = df_scaled[i]*df_reference[i].std() + df_reference[i].mean()\n",
        "  #return df_scaled"
      ],
      "metadata": {
        "id": "yvtWO85roBUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##normalize(df_for_norm, df_reference)"
      ],
      "metadata": {
        "id": "533CHpps0heS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(df_for_norm, df_reference):\n",
        "  if (any(df_for_norm.columns != df_reference.columns)):\n",
        "    print('normalize function: Dataframes com colunas diferentes')\n",
        "    return\n",
        "  \n",
        "  print(f'normalize function: columns_reference: {df_reference.columns}')\n",
        "  for i in df_for_norm.columns: \n",
        "    df_for_norm[i] = (df_for_norm[i] - df_reference[i].min()) / (df_reference[i].max() - df_reference[i].min())\n",
        "  #return df_for_norm"
      ],
      "metadata": {
        "id": "2kQ9h83s0e3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##undo_normalize(df_for_norm, df_reference)"
      ],
      "metadata": {
        "id": "uA03xf8y1IzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def undo_normalize(df_normalized, df_reference):\n",
        "  if (any(df_normalized.columns != df_reference.columns)):\n",
        "    print('undo_normalize function: Dataframes com colunas diferentes')\n",
        "    return\n",
        "  \n",
        "  print(f'undo_normalize function: columns_reference: {df_reference.columns}')\n",
        "  for i in df_normalized.columns: \n",
        "    df_normalized[i] = df_normalized[i]*(df_reference[i].max() - df_reference[i].min()) + df_reference[i].min()\n",
        "  #return df_normalized"
      ],
      "metadata": {
        "id": "PDLjbRE21IzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##str2int_simple_encoder(df,columns='all')"
      ],
      "metadata": {
        "id": "MNDQkSx6ASv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def str2int_simple_encoder(df,columns='all'):\n",
        "  import pandas as pd\n",
        "  \n",
        "  id_dict = {}\n",
        "  if (columns=='all'):\n",
        "    \n",
        "    for i in df.columns:\n",
        "      if (df[i].dtype==object):\n",
        "        id_dict[i] = {}\n",
        "        unique_values = df[i].unique()\n",
        "        id_dict[i] = {name: id + 1 for id, name in enumerate(unique_values)}\n",
        "\n",
        "        df[i] = df[i].apply(lambda row, value : value[row], value = id_dict[i] )\n",
        "\n",
        "  else:\n",
        "    \n",
        "    for i in columns:\n",
        "      if ( (df[i].dtype==object) and (i in df.columns) ):\n",
        "        id_dict[i] = {}\n",
        "        unique_values = df[i].unique()\n",
        "        id_dict[i] = {name: id + 1 for id, name in enumerate(unique_values)}\n",
        "\n",
        "        df[i] = df[i].apply(lambda row, value : value[row], value = id_dict[i] )\n",
        "      else:\n",
        "        print('str2int_simple_encoder: coluna especificada não é do tipo \"object\" ou não existe no dataframe')\n",
        "        return\n",
        "  \n",
        "  return id_dict"
      ],
      "metadata": {
        "id": "sC623raeAQvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#como era feito anteriormente:\n",
        "'''\n",
        "df = dados\n",
        "\n",
        "#Criar os dicionários para os valores únicos das colunas categóricas\n",
        "type_id = {}\n",
        "aux = df['type'].unique()\n",
        "for i in aux:\n",
        "  type_id[i] = np.where(aux==i)[0][0]+1\n",
        "print(type_id)\n",
        "\n",
        "#converter os valores categóricos da coluna \"type\" por numéricos\n",
        "df['type'] = df['type'].apply(lambda row, value : value[row],\n",
        "                                                    value = type_id )\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "NQ-4x_yBAqdL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "04815107-4c94-4467-efa4-8387b59cc3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndf = dados\\n\\n#Criar os dicionários para os valores únicos das colunas categóricas\\ntype_id = {}\\naux = df[\\'type\\'].unique()\\nfor i in aux:\\n  type_id[i] = np.where(aux==i)[0][0]+1\\nprint(type_id)\\n\\n#converter os valores categóricos da coluna \"type\" por numéricos\\ndf[\\'type\\'] = df[\\'type\\'].apply(lambda row, value : value[row],\\n                                                    value = type_id )\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##str2int_hot_encoder(df,columns='all')"
      ],
      "metadata": {
        "id": "3lwimYi9Tce_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def str2int_hot_encoder(df,columns='all'):\n",
        "  import pandas as pd\n",
        "  \n",
        "  id_dict = {}\n",
        "  if (columns=='all'):\n",
        "    \n",
        "    for i in df.columns:\n",
        "      if (df[i].dtype==object):\n",
        "        id_dict[i] = {}\n",
        "        unique_values = df[i].unique()\n",
        "        for id,name in enumerate(unique_values):\n",
        "          aux = [0]*(len(unique_values)-1)\n",
        "          aux.insert(id,1)\n",
        "          id_dict[i][name] = aux\n",
        "\n",
        "        df[i] = df[i].apply(lambda row, value : value[row], value = id_dict[i] )\n",
        "\n",
        "  else:\n",
        "    \n",
        "    for i in columns:\n",
        "      if ( (df[i].dtype==object) and (i in df.columns) ):\n",
        "        id_dict[i] = {}\n",
        "        unique_values = df[i].unique()\n",
        "        for id,name in enumerate(unique_values):\n",
        "          aux = [0]*(len(unique_values)-1)\n",
        "          aux.insert(id,1)\n",
        "          id_dict[i][name] = aux\n",
        "\n",
        "        df[i] = df[i].apply(lambda row, value : value[row], value = id_dict[i] )\n",
        "      else:\n",
        "        print('str2int_simple_encoder: coluna especificada não é do tipo \"object\" ou não existe no dataframe')\n",
        "        return\n",
        "  \n",
        "  return id_dict"
      ],
      "metadata": {
        "id": "MPjBF7PvTZpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##validation_curve_change_param(model,x_train,y_train,parameters = {})"
      ],
      "metadata": {
        "id": "bllTD27nY5by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_curve_change_param(model,\n",
        "                                  x_train,\n",
        "                                  y_train,\n",
        "                                  parameters = {}, # definindo os valores de parâmetros a serem testados\n",
        "                                  ylim=None\n",
        "                                  ):\n",
        "  '''\n",
        "  Example:\n",
        "  parameters = {'C': np.arange(10000, 100000, 10000),\n",
        "              'epsilon': [ 1, 5, 10,100,200,300,400,500,600],\n",
        "              'tol': [0.001,0.01,0.1,1,5,10,100,1000,2000],\n",
        "              'gamma': np.arange(0.01, 1.2, 0.05), \n",
        "              }\n",
        "  validation_curve_change_param(model = SVR(kernel='rbf'),\n",
        "                                x_train = x_train,\n",
        "                                y_train = y_train,\n",
        "                                parameters = parameters\n",
        "                                )\n",
        "  '''\n",
        "  # Lista para armazenar os valores \"Y\" para cada hiperparâmetro\n",
        "  lista_train_scores_mean = []\n",
        "  lista_train_scores_std = []\n",
        "  lista_test_scores_mean = []\n",
        "  lista_test_scores_std = []\n",
        "\n",
        "  count = 0\n",
        "  for key, value in parameters.items():\n",
        "    \n",
        "\n",
        "\n",
        "    # calculando a curva de validação\n",
        "    train_scores, test_scores = sklearn.model_selection.validation_curve(\n",
        "        model, x_train, y_train, \n",
        "        param_name=key, \n",
        "        param_range=value,\n",
        "        scoring=\"r2\", \n",
        "        n_jobs=-1\n",
        "        )\n",
        "\n",
        "    # médias e desvios-padrão dos resultados da validação cruzada (para cada ponto da curva)\n",
        "    lista_train_scores_mean.append( np.mean(train_scores, axis=1) )\n",
        "    lista_train_scores_std.append( np.std(train_scores, axis=1) )\n",
        "    lista_test_scores_mean.append( np.mean(test_scores, axis=1) )\n",
        "    lista_test_scores_std.append( np.std(test_scores, axis=1) )\n",
        "\n",
        "  \n",
        "    ## GRAFICO ##\n",
        "    plt.subplots(1,1, \n",
        "                 #sharex = True, sharey = True\n",
        "                 )\n",
        "\n",
        "    # plotando curva correspondente ao treino\n",
        "    plt.plot(value, #https://matplotlib.org/stable/tutorials/introductory/pyplot.html\n",
        "             lista_train_scores_mean[count],\n",
        "             '.-',\n",
        "             label='Treino')\n",
        "    plt.fill_between(value,\n",
        "                     lista_train_scores_mean[count] - lista_train_scores_std[count],\n",
        "                     lista_train_scores_mean[count] + lista_train_scores_std[count],\n",
        "                     alpha=0.1)\n",
        "    \n",
        "    # plotando curva correspondente ao teste\n",
        "    plt.plot(value, \n",
        "             lista_test_scores_mean[count],\n",
        "             '.-',\n",
        "             label='Teste')\n",
        "    plt.fill_between(value, \n",
        "                     lista_test_scores_mean[count] - lista_test_scores_std[count],\n",
        "                     lista_test_scores_mean[count] + lista_test_scores_std[count],\n",
        "                     alpha=0.1)\n",
        "\n",
        "    # formatando gráfico\n",
        "    if (count==0): plt.title('Curva de Validação')\n",
        "    plt.xlabel(key)\n",
        "    plt.ylabel(\"$R^2$\")\n",
        "    plt.legend(loc=\"best\");\n",
        "    plt.ylim(ylim)\n",
        "    ## GRAFICO ##\n",
        "    \n",
        "    count +=1"
      ],
      "metadata": {
        "id": "tnfeck5uY2-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##display_score(m,x_train,x_test,y_train,y_test))"
      ],
      "metadata": {
        "id": "OHgl4ej_o2wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out-of-bag parameter:\n",
        "\n",
        "https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2020/12/out-of-bag-oob-score-in-the-random-forest-algorithm/\n",
        "\n",
        "https://stats.stackexchange.com/questions/88980/why-on-average-does-each-bootstrap-sample-contain-roughly-two-thirds-of-observat\n",
        "\n",
        "https://stats.stackexchange.com/questions/198839/evaluate-random-forest-oob-vs-cv"
      ],
      "metadata": {
        "id": "lgi1bQa7SxAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(v_real,v_pred): \n",
        "    return np.sqrt(sklearn.metrics.mean_squared_error(v_real,v_pred)) #leia sobre sklearn.metrics.mean_squared_error\n",
        "def r2(v_real,v_pred): \n",
        "    return sklearn.metrics.r2_score(v_real,v_pred) #leia sobre sklearn.metrics.r2_score\n",
        "\n",
        "##função para avaliar RMSE, R2 e OOB_score\n",
        "def display_score(m,x_train,x_test,y_train,y_test):\n",
        "    \n",
        "    res = [[rmse( y_train,m.predict(x_train) ), r2( y_train,m.predict(x_train) )],\n",
        "          [rmse( y_test,m.predict(x_test) ), r2( y_test,m.predict(x_test) )]] #a função display score irá retornar uma tabela\n",
        "    \n",
        "    score = pd.DataFrame(res, columns=['RMSE','R2'], index = ['Treino','Teste'])\n",
        "\n",
        "    if hasattr(m, 'oob_score_'): #https://www.programiz.com/python-programming/methods/built-in/hasattr\n",
        "        score.loc['OOB'] = [rmse(y_train, m.oob_prediction_), m.oob_score_]\n",
        "\n",
        "    display(score)"
      ],
      "metadata": {
        "id": "T5sUq8jfoPXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##plot_permutation_importance( model, x_val, y_val, x_val_columns )"
      ],
      "metadata": {
        "id": "k_j36m1q3rmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_permutation_importance( model, x_val, y_val, x_val_columns ):\n",
        "  #https://medium.com/horadecodar/gr%C3%A1ficos-de-barra-com-matplotlib-85628bfc4351#:~:text=barh()%3A,os%20seguintes%20par%C3%A2metros%3A\n",
        "  from sklearn.inspection import permutation_importance\n",
        "\n",
        "  r = permutation_importance(model, x_val, y_val,\n",
        "                             n_repeats=30,\n",
        "                             scoring='r2',\n",
        "                             random_state=0)\n",
        "\n",
        "  df = pd.DataFrame( columns=['mean','std'] )\n",
        "\n",
        "  for i in r.importances_mean.argsort()[::-1]:\n",
        "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
        "      #print(f\"{x_val_columns[i]:<20}\"\n",
        "      #      f\"{r.importances_mean[i]:.3f}\"\n",
        "      #      f\" +/- {r.importances_std[i]:.3f}\")\n",
        "      df.loc[x_val_columns[i]] = [r.importances_mean[i], r.importances_std[i]]\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "zzKBiry63qqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gráficos"
      ],
      "metadata": {
        "id": "6RzP6hKOX7wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plt_valuecounts_by(df,variable,by)"
      ],
      "metadata": {
        "id": "QM6DjLSbYA9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_valuecounts_by(df = pd.DataFrame(),\n",
        "                       variable = [],\n",
        "                       by = '',\n",
        "                       consider_none = False,\n",
        "                       consider_zeros = False):\n",
        "  '''\n",
        "  Plotar a qtd de valores existentes da variavel=\"variable\" para cada classe da variável \"by\"\n",
        "  '''\n",
        "  \n",
        "  #https://plotly.com/python/histograms/\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  fig = go.Figure()\n",
        "\n",
        "  #Filtrar apenas as linha que possuem dados de \"variavel\"\n",
        "  for i in variable:\n",
        "    \n",
        "    if (consider_none==False):\n",
        "      df.loc[ df[i]=='none', i ] = np.nan\n",
        "      df.loc[ df[i]=='None', i ] = np.nan\n",
        "\n",
        "    if (consider_zeros==False):\n",
        "      df.loc[ df[i]==0, i ] = np.nan\n",
        "\n",
        "    df_aux = df.loc[ df[i].notna(), by]\n",
        "\n",
        "    fig.add_trace(go.Histogram(\n",
        "        x=df_aux,\n",
        "        histnorm='',\n",
        "        name=i, # name used in legend and hover labels\n",
        "        #marker_color='#EB89B5',\n",
        "        #opacity=0.75\n",
        "        ))\n",
        "\n",
        "  fig.update_layout(\n",
        "      title_text=f'Quantity of data by each {by}', # title of plot\n",
        "      xaxis_title_text=by, # xaxis label\n",
        "      yaxis_title_text='Count', # yaxis label\n",
        "      bargap=0.2, # gap between bars of adjacent location coordinates\n",
        "      bargroupgap=0.1 # gap between bars of the same location coordinates\n",
        "      )\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "Ub_A9LvesfM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plt_hist_of_columns(df)"
      ],
      "metadata": {
        "id": "OwGZHhgGsi9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_hist_of_columns(df):\n",
        "  # converter colunas numéricas para \"float\"\n",
        "  float_df_columns = []\n",
        "  for i in df.columns:\n",
        "    try:\n",
        "      df[i] = df[i].astype(float)\n",
        "    except:\n",
        "      print(f'heatmap_pearson function: X column \"{i}\" is a {df[i].dtype}')\n",
        "    else:\n",
        "      float_df_columns += [i]\n",
        "  \n",
        "  aux = []\n",
        "  for i in float_df_columns:\n",
        "    aux += [i]\n",
        "    if (len(aux)==4):\n",
        "      try:\n",
        "        df[aux] = df[aux].astype(float)\n",
        "      except:\n",
        "        print()\n",
        "      df[aux].hist()\n",
        "      aux = []\n",
        "  df[aux].hist()"
      ],
      "metadata": {
        "id": "JexNdshlsk3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### heatmap_corr(df, x='all', y='all')"
      ],
      "metadata": {
        "id": "hu87rosewxW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heatmap_corr(df,\n",
        "                 x='all', \n",
        "                 y='all',\n",
        "                 method='pearson', \n",
        "                 min_periods=1,\n",
        "                 color='di'):\n",
        "  import pandas as pd\n",
        "  \n",
        "  corr_pear = df.corr( min_periods=min_periods, method=method )\n",
        "  \n",
        "  if (x=='all'):\n",
        "    x = corr_pear.columns.tolist()\n",
        "  #print(x)\n",
        "  if (y=='all'):\n",
        "    y = corr_pear.columns.tolist()\n",
        "  #print(y)\n",
        "  heatmap_pearson = pd.DataFrame( columns=x, index=y )\n",
        "  heatmap_pearson = corr_pear.loc[y,x]\n",
        "  \n",
        "    #GRAFICO#\n",
        "  f, ax = plt.subplots(figsize=( 1*len(x)+3, 1*len(y) ))\n",
        "  if color=='mono':\n",
        "    colors = ('#00076e', '#1b00ff', '#d0cbff', '#FFFFFF', '#d0cbff', '#1b00ff', '#00076e')\n",
        "  elif (color=='di'):\n",
        "    colors = ('#7e0000', '#ff0000', '#fecfcf', '#FFFFFF', '#d0cbff', '#1b00ff', '#00076e')\n",
        "  cmap = sns.blend_palette(colors, input='rgb', as_cmap=True)\n",
        "  sns.heatmap(heatmap_pearson, annot=True, cmap=cmap, ax=ax, center=0) \n",
        "\n",
        "  return heatmap_pearson"
      ],
      "metadata": {
        "id": "cXG76USmwu2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### heatmap_pearson(df='pandas_DataFrame', x='all', y='all', allow_duplicates=True, color='di', graphic='coeff')"
      ],
      "metadata": {
        "id": "xab3pxC6TtMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def heatmap_pearson(df='pandas_DataFrame',\n",
        "                    x='all',\n",
        "                    y='all',\n",
        "                    allow_duplicates=True,\n",
        "                    color='di', \n",
        "                    graphic='coeff'):\n",
        "  from scipy.stats import pearsonr\n",
        "  '''\n",
        "  Returns a heatmap plot with pearson's coefficients or their p-values.\n",
        "\n",
        "  df      = Dataframe\n",
        "  x and y = list of x and y heatmap columns/axis\n",
        "  color   = \"di\" or \"mono\"\n",
        "  graphic = \"coeff\" or \"pvalue\"\n",
        "  '''\n",
        "  if (x=='all'):\n",
        "    x = df.columns\n",
        "\n",
        "  # converter colunas numéricas para \"float\"\n",
        "  float_df_columnsx = []\n",
        "  for i in x:\n",
        "    try:\n",
        "      df[i] = df[i].astype(float)\n",
        "    except:\n",
        "      print(f'heatmap_pearson function: X column \"{i}\" is a {df[i].dtype}')\n",
        "    else:\n",
        "      float_df_columnsx += [i]\n",
        "  x = float_df_columnsx\n",
        "\n",
        "\n",
        "  if (y=='all'):\n",
        "    y = df.columns\n",
        "\n",
        "  # converter colunas numéricas para \"float\"\n",
        "  float_df_columnsy = []\n",
        "  for i in y:\n",
        "    try:\n",
        "      df[i] = df[i].astype(float)\n",
        "    except:\n",
        "      print(f'heatmap_pearson function: Y column \"{i}\" is a {df[i].dtype}')\n",
        "    else:\n",
        "      float_df_columnsy += [i]\n",
        "  y = float_df_columnsy\n",
        "  \n",
        "\n",
        "  pear_heatmap = pd.DataFrame( columns=x, index=y, dtype=float)\n",
        "  pvalue_heatmap = pd.DataFrame( columns=x, index=y, dtype=float )\n",
        "  for i in x: #columns\n",
        "    #print(i)\n",
        "    for j in y: #index\n",
        "      df_aux = df[[i,j]].dropna()\n",
        "      \n",
        "      #Retirar duplicadas nas coordenadas de \"df_aux\"\n",
        "      if (allow_duplicates==False):\n",
        "        old_df = df_aux\n",
        "        new_df = pd.DataFrame( columns=[i,j] )\n",
        "        lines_new_df = []\n",
        "        for index in old_df.index:\n",
        "          line_old_df = f'{df.loc[index,i]} {df.loc[index,j]}'\n",
        "          if not(line_old_df in lines_new_df):\n",
        "            lines_new_df += [line_old_df]\n",
        "            new_df.loc[index, [i,j] ] = old_df.loc[index, [i,j]]\n",
        "        df_aux = new_df\n",
        "\n",
        "      if (df_aux.shape[0]==0):\n",
        "        pear_heatmap.loc[j,i], pvalue_heatmap.loc[j,i] = (np.nan, np.nan)\n",
        "        continue\n",
        "      columnx = df_aux.iloc[:,0]\n",
        "      columny = df_aux.iloc[:,1]\n",
        "      #print(columnx.shape, columny.shape)\n",
        "      pear_heatmap.loc[j,i], pvalue_heatmap.loc[j,i] = pearsonr( columnx, columny )\n",
        "\n",
        "  ## GRAFICO ##\n",
        "  if (graphic=='coeff'):\n",
        "    graphic = pear_heatmap\n",
        "  elif (graphic=='pvalue'):\n",
        "    graphic = pvalue_heatmap\n",
        "\n",
        "  f, ax = plt.subplots(figsize=( 1*len(x)+3, 1*len(y) ))\n",
        "  if color=='mono':\n",
        "    colors = ('#00076e', '#1b00ff', '#d0cbff', '#FFFFFF', '#d0cbff', '#1b00ff', '#00076e')\n",
        "  elif (color=='di'):\n",
        "    colors = ('#7e0000', '#ff0000', '#fecfcf', '#FFFFFF', '#d0cbff', '#1b00ff', '#00076e')\n",
        "  cmap = sns.blend_palette(colors, input='rgb', as_cmap=True)\n",
        "  sns.heatmap(graphic, annot=True, cmap=cmap, ax=ax, center=0)\n",
        "\n",
        "  return (pear_heatmap, pvalue_heatmap)"
      ],
      "metadata": {
        "id": "5OETMwBQThqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot_stacked_hist_or_bar_by(df,variable='',by='',mode='bar or hist')"
      ],
      "metadata": {
        "id": "UaT16sc9fvXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# Histogramas empilhados por classe \"by\"\n",
        "def plot_stacked_hist_or_bar_by(df, variable = '',\n",
        "                                by = '',\n",
        "                                mode = 'bar or hist', \n",
        "                                alpha = 0.3,\n",
        "                                bins_hist = 1, \n",
        "                                width_bar = 'default',\n",
        "                                bar_norm = False,\n",
        "                                colors_reference = ['b','g','r','c','m','y','k'],\n",
        "                                figsize = [6.4, 4.8]):\n",
        "  '''\n",
        "  Exemplos semelhantes de grafico de barras: https://matplotlib.org/stable/gallery/lines_bars_and_markers/bar_stacked.html#sphx-glr-gallery-lines-bars-and-markers-bar-stacked-py\n",
        "  \n",
        "  Essa função se baseia na criação de dicionários para os valores da base (variável \"bottom\"\n",
        "  do matplotlib) e das frequencias (variável \"height\" do matplotlib). As chaves(keys) desses\n",
        "  dicionários são os valores únicos de \"df[variable]\" e os valores são referentes à frequencia\n",
        "  (obtidas a partir da função \"value_counts\").\n",
        "\n",
        "  Essa função retorna \"fig\" e \"ax\" do Matplotlib. Portanto, o gráfico criado pode ser editado\n",
        "  posteriormente, mesmo com certa limitação.\n",
        "\n",
        "  O modo 'hist' serve apenas para quando \"variable\" possui valores numéricos.\n",
        "  '''\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  df.dropna(subset=[variable], inplace=True)\n",
        "  x_range = [df[variable].min(), df[variable].max()]\n",
        "\n",
        "  unique_by = df[by].unique()\n",
        "  #Definir as CORES para cada valor único da variável \"by\".\n",
        "  #Caso houver mais valores que o tamanho de \"colors_reference\", as cores serão repetidas:\n",
        "  count, colors = (0, [])\n",
        "  for i in range(len(unique_by)):\n",
        "    if (count==len(colors_reference)):\n",
        "      count = 0\n",
        "    colors += [colors_reference[count]]\n",
        "    count += 1\n",
        "  \n",
        "  #dicionario com os valores da base:\n",
        "  unique_variable = df[variable].unique()\n",
        "  bottoms = dict(zip(unique_variable,len(unique_variable)*[0]))\n",
        "  #variavel referencia - dicionario com valores zerados:\n",
        "  values_0 = copy.copy(bottoms) \n",
        "\n",
        "  #Histograma - o dicionário para o \"x\", \"height\" e \"bottom\" é diferente quando se deseja construir um histograma:\n",
        "  #             faz-se necessário trabalhar com numeros/floats\n",
        "  if (mode=='hist'):\n",
        "    histogram_width = (x_range[1]-x_range[0])/bins_hist\n",
        "    histogram_x = np.arange( x_range[0], x_range[1], histogram_width ).tolist()\n",
        "    histogram_bottoms = dict(zip(histogram_x,len(histogram_x)*[0])) ##valores com o valor da base\n",
        "    histogram_0 = copy.copy(histogram_bottoms) #variavel referencia - dicionario com valores zerados\n",
        "\n",
        "  #Definir variável com o valor total de frequência para cada \"variable\"\n",
        "  if (bar_norm==True):\n",
        "    total_value_counts = df[variable].value_counts()\n",
        "\n",
        "  count = 0\n",
        "  for i in unique_by:\n",
        "    values = copy.copy(values_0) #armazenará os \"value_counts\" referentes a vada valor unico de uma variavel\n",
        "    \n",
        "    #Obter os value counts de cada \"variable\" para cada \"unique_by\"\n",
        "    df_filtrado = df.loc[df[by]==i, variable]\n",
        "    hist_aux_df = df_filtrado.value_counts()\n",
        "    for j in hist_aux_df.index:\n",
        "      if (bar_norm==True):\n",
        "        values[j] += hist_aux_df[j]/total_value_counts[j] #Normalizar \"values\" pelo valor total\n",
        "      else:\n",
        "        values[j] += hist_aux_df[j]\n",
        "\n",
        "\n",
        "    if (mode=='bar'):\n",
        "      ## Grafico ##\n",
        "      if (width_bar=='default') and not(isinstance(x_range[1], str)): width_bar = (x_range[1]-x_range[0])/len(unique_variable)\n",
        "      if isinstance(x_range[1], str): width_bar=0.8\n",
        "\n",
        "      ax.bar(x=list(values.keys()), #https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.bar.html#matplotlib.axes.Axes.bar\n",
        "            height=list(values.values()),\n",
        "            width=width_bar,\n",
        "            bottom=list(bottoms.values()),\n",
        "            align = 'center',\n",
        "            color = colors[count],\n",
        "            #edgecolor='black',\n",
        "            #hatch='//',\n",
        "            alpha=alpha,\n",
        "            label= i\n",
        "            )\n",
        "      ## Grafico ##\n",
        "      for j in bottoms.keys():\n",
        "        bottoms[j] += values[j]\n",
        "    \n",
        "    elif (mode=='hist'):\n",
        "      histogram_values = copy.copy(histogram_0)\n",
        "      for j in values.keys():\n",
        "        for k in histogram_x[::-1]:\n",
        "          if (j>=k):\n",
        "            histogram_values[k] += values[j]\n",
        "            break\n",
        "      ## Grafico ##\n",
        "      ax.bar(x=list(histogram_values.keys()), #https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.bar.html#matplotlib.axes.Axes.bar\n",
        "            height=list(histogram_values.values()),\n",
        "            width=histogram_width,\n",
        "            bottom=list(histogram_bottoms.values()),\n",
        "            align = 'edge',\n",
        "            color = colors[count],\n",
        "            #edgecolor='black',\n",
        "            #hatch='//',\n",
        "            alpha=alpha,\n",
        "            label= i\n",
        "            )\n",
        "      ## Grafico ##\n",
        "      for j in histogram_bottoms.keys():\n",
        "        histogram_bottoms[j] += histogram_values[j]\n",
        "    \n",
        "    count += 1\n",
        "  \n",
        "\n",
        "  ## Grafico ##\n",
        "  ax.set(#title=f'Frequência de {variable} por tipo de membrana',\n",
        "         xlabel=variable,\n",
        "         ylabel='Frequência')\n",
        "  plt.legend();\n",
        "  ax.margins(0.05)\n",
        "  ## Grafico ##\n",
        "\n",
        "  print('min =',df[variable].min())\n",
        "  print('max =',df[variable].max())\n",
        "  \n",
        "  return fig, ax"
      ],
      "metadata": {
        "id": "U1uSSHQRfsQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###compare_hists_by(df1, df2, variable = '', by = '')"
      ],
      "metadata": {
        "id": "ZIhaBf_IU7LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def compare_hists_by(df1, df2,\n",
        "                     variable = '',\n",
        "                     by = '',\n",
        "                     df1_name = 'default',\n",
        "                     df2_name = 'default',\n",
        "                     alpha = 0.7,\n",
        "                     bins_hist = 1,\n",
        "                     colors_reference = ['b','g','r','c','m','y','k'],\n",
        "                     figsize = [12.8, 7.2]):\n",
        "  '''\n",
        "  \"variable\" precisa ter valores numéricos.\n",
        "\n",
        "  Essa função retorna \"fig\" e \"ax\" do Matplotlib. Portanto, o gráfico criado pode ser editado\n",
        "  posteriormente, mesmo com certa limitação.\n",
        "  \n",
        "  A lógica dessa função foi copiada da função \"plot_stacked_hist_or_bar_by\".\n",
        "  Para melhor compreender esse cógido, ler antes o código da função \"plot_stacked_hist_or_bar_by\"\n",
        "  '''\n",
        "  fig, ax = plt.subplots( 2, 2, figsize=figsize )\n",
        "  df1.dropna(subset=[variable], inplace=True)\n",
        "  df2.dropna(subset=[variable], inplace=True)\n",
        "  \n",
        "  x_range_min = min(df1[variable].min(), df2[variable].min())\n",
        "  x_range_max = max(df1[variable].max(), df2[variable].max())\n",
        "\n",
        "  #Achar os valores únicos da variável \"by\" presentes nos dois dataframes (df1 e df2)\n",
        "  unique_by = (df1[by].unique().tolist() + df2[by].unique().tolist())\n",
        "  unique_by = list(dict.fromkeys(unique_by))\n",
        "  unique_by.sort()\n",
        "  #Definir as cores para cada valor único da variável \"by\".\n",
        "  #Caso houver mais valores que o tamanho de \"colors_reference\", as cores serão repetidas:\n",
        "  count, colors = (0, [])\n",
        "  for i in range(len(unique_by)):\n",
        "    if (count==len(colors_reference)):\n",
        "      count = 0\n",
        "    colors += [colors_reference[count]]\n",
        "    count += 1\n",
        "  \n",
        "  #Histograma - o dicionário para o \"x\", \"height\" e \"bottom\" para plotar um histograma\n",
        "  histogram_width = (x_range_max-x_range_min)/bins_hist\n",
        "  histogram_x = np.arange( x_range_min, x_range_max, histogram_width ).tolist()\n",
        "  histogram_bottoms = dict(zip(histogram_x,len(histogram_x)*[0])) ##valores com o valor da base\n",
        "  histogram_0 = copy.copy(histogram_bottoms) #variavel referencia - dicionario com valores zerados\n",
        "\n",
        "  ## Plots individuais ##\n",
        "  histogram_df = {}\n",
        "  for (df, position) in [(df1, 0), (df2, 1)]:\n",
        "    unique_variable = df[variable].unique()\n",
        "    #variavel referencia - dicionario com valores zerados:\n",
        "    values_0 = dict(zip(unique_variable,len(unique_variable)*[0]))\n",
        "\n",
        "    count = 0\n",
        "    for i in unique_by:\n",
        "      values = copy.copy(values_0) #armazenará os \"value_counts\" referentes a vada valor unico de uma variavel\n",
        "      aux_df = df.loc[df[by]==i, variable]\n",
        "      hist_aux_df = aux_df.value_counts()\n",
        "      for j in hist_aux_df.index:\n",
        "        values[j] += hist_aux_df[j]\n",
        "      \n",
        "      ## Histograma ##\n",
        "      histogram_values = copy.copy(histogram_0)\n",
        "      for j in values.keys():\n",
        "        for k in histogram_x[::-1]:\n",
        "          if (j>=k):\n",
        "            histogram_values[k] += values[j]\n",
        "            break\n",
        "\n",
        "      ## Grafico ##\n",
        "      ax[position,0].bar(x=list(histogram_values.keys()), #https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.bar.html#matplotlib.axes.Axes.bar\n",
        "                        height=list(histogram_values.values()),\n",
        "                        width=histogram_width,\n",
        "                        bottom=list(histogram_bottoms.values()),\n",
        "                        align = 'edge',\n",
        "                        color = colors[count],\n",
        "                        #edgecolor='black',\n",
        "                        #hatch='//',\n",
        "                        alpha=alpha,\n",
        "                        label= i\n",
        "                        )\n",
        "      if (position==0):\n",
        "        if (df1_name=='default'):\n",
        "          title='df1'\n",
        "        else:\n",
        "          title=df1_name\n",
        "      if (position==1):\n",
        "        if (df2_name=='default'):\n",
        "          title='df2'\n",
        "        else:\n",
        "          title=df2_name\n",
        "      ax[position,0].set_title(title)\n",
        "      ax[position,0].set_ylabel('Frequência')\n",
        "      if (position==0): ax[position,0].set_xticks([]) #ocultar o eixo x\n",
        "      if (position==1): ax[position,0].set_xlabel(variable)\n",
        "      ax[position,0].legend();\n",
        "      #ax[position,0].margins(0.05)\n",
        "      ## Grafico ##\n",
        "\n",
        "      for j in histogram_bottoms.keys():\n",
        "        histogram_bottoms[j] += histogram_values[j]\n",
        "      \n",
        "      count += 1\n",
        "    \n",
        "    histogram_df[position] = histogram_bottoms.copy() #Armazenar valor para depois plotar junto o df1 e df2\n",
        "    histogram_bottoms = histogram_0.copy()\n",
        "\n",
        "  ## Plots juntos ##\n",
        "  ## Grafico ##\n",
        "  ax = plt.subplot(122)\n",
        "  for i in (0, 1):\n",
        "    if (i==0):\n",
        "      color = 'darkred'\n",
        "      if (df1_name=='default'):\n",
        "        label='df1'\n",
        "      else:\n",
        "        label=df1_name\n",
        "    if (i==1):\n",
        "      color = 'cornflowerblue'\n",
        "      if (df2_name=='default'):\n",
        "        label='df2'\n",
        "      else:\n",
        "        label=df2_name\n",
        "    ax.bar(x=list(histogram_df[i].keys()), #https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.bar.html#matplotlib.axes.Axes.bar\n",
        "                  height=list(histogram_df[i].values()),\n",
        "                  width=histogram_width,\n",
        "                  align = 'edge',\n",
        "                  color = color,\n",
        "                  #edgecolor='black',\n",
        "                  #hatch='//',\n",
        "                  alpha=alpha,\n",
        "                  label= label\n",
        "                  )\n",
        "    ax.legend()\n",
        "    ax.set(xlabel=variable, ylabel='Frequência')\n",
        "   ## Grafico ##\n",
        "\n",
        "  return fig, ax"
      ],
      "metadata": {
        "id": "fHv7eLOsU31R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhlmbCW0tt1h"
      },
      "source": [
        "###draw_tree(t, dados, size=10, ratio=1, precision=0) - Modelo RF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_tree(t, dados, size=10, ratio=1, precision=0):\n",
        "   \n",
        "    import re\n",
        "    import graphviz\n",
        "    import sklearn.tree\n",
        "    import IPython.display\n",
        "    \n",
        "    s=sklearn.tree.export_graphviz(t, out_file=None, feature_names=dados.columns, filled=True,\n",
        "                                   special_characters=True, rotate=True, precision=precision)\n",
        "    IPython.display.display(graphviz.Source(re.sub('Tree {',\n",
        "       f'Tree {{ size={size}; ratio={ratio}', s)))"
      ],
      "metadata": {
        "id": "HwlEYRr-kXia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###plotar_importancias(modelo, tags, n=10) - modelo RF"
      ],
      "metadata": {
        "id": "gqluL2em-ubQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotar_importancias(modelo, tags, n=10):\n",
        "    \n",
        "    fig, ax = plt.subplots(1,2, figsize = (20,4))\n",
        "\n",
        "    coefs = []\n",
        "    abs_coefs = []\n",
        "\n",
        "    if hasattr(modelo,'coef_'):\n",
        "        imp = modelo.coef_\n",
        "    elif hasattr(modelo,'feature_importances_'):\n",
        "        imp = modelo.feature_importances_\n",
        "    else:\n",
        "        print('sorry, nao vai rolar!')\n",
        "        return\n",
        "\n",
        "    coefs = (pd.Series(imp, index = tags))\n",
        "    coefs.plot(use_index=False, ax=ax[0]);\n",
        "    abs_coefs = (abs(coefs)/(abs(coefs).sum()))\n",
        "    abs_coefs.sort_values(ascending=False).plot(use_index=False, ax=ax[1],marker='.')\n",
        "\n",
        "    ax[0].set_title('Importâncias relativas das variáveis')\n",
        "    ax[1].set_title('Importâncias relativas das variáveis - ordem decrescente')\n",
        "\n",
        "    abs_coefs_df = pd.DataFrame(np.array(abs_coefs).T,\n",
        "                                columns = ['Importancias'],\n",
        "                                index = tags)\n",
        "\n",
        "    df = abs_coefs_df['Importancias'].sort_values(ascending=False)\n",
        "    \n",
        "    print(df.iloc[0:n])\n",
        "    plt.figure()\n",
        "    df.iloc[0:n].plot(kind='barh', figsize=(15,0.25*n), legend=False)\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "D-DlUSMwp5LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Como exportar e importar com pickle"
      ],
      "metadata": {
        "id": "qucVwpdpPt_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "import pickle\n",
        "```\n",
        "\n",
        "Exportar:\n",
        "\n",
        "```\n",
        "with open( 'example_file.pkl', 'wb' ) as f:\n",
        "  pickle.dump( \"objetos/variáveis\", f )\n",
        "```\n",
        "\n",
        "Importar\n",
        "\n",
        "```\n",
        "with open( 'example_file.pkl', 'rb' ) as f:\n",
        "  \"objetos/variáveis\" = pickle.load( f )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "If_S_7Z1QA7_"
      }
    }
  ]
}